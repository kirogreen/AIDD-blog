+++
date = '2025-10-10T13:15:06+08:00'
draft = false
title = "Bayesian Flow Networks Enable Unified Protein and Antibody Generation"
authors = "[Michael Yu]"
categories = ["literature review"]
tags = ['protein language models', 'de novo design', 'antibody', 'generative AI', 'Bayesian Flow Networks']
description = "daily summary of latest AIDD literature"
+++


1.  **Summary:**
    - **Title:** Protein sequence modelling with Bayesian flow networks
    - **Journal:** Nature Communications
    - **Publication Date:** 03 April 2025
    - **DOI:** 10.1038/s41467-025-58250-2
    - **Primary Research Institution:** InstaDeep, London, England
    - **Abstract:** This work introduces ProtBFN, a 650M parameter Bayesian Flow Network (BFN) model for protein sequence generation. Trained on curated UniProtKB sequences, it outperforms autoregressive (e.g., ProtGPT2) and discrete diffusion (e.g., EvoDiff) baselines in generating diverse, novel, and structurally coherent proteins. Fine-tuned on antibody heavy chains (AbBFN), it achieves competitive zero-shot conditional generation (e.g., CDR inpainting) compared to specialized BERT-style models, demonstrating BFNs' flexibility for both unconditional and conditional tasks without task-specific training.

2.  **Background:**
    The vast combinatorial space of possible protein sequences remains largely unexplored, limiting our understanding of biology and hindering the design of novel therapeutic proteins. Machine learning, particularly methods inspired by natural language processing (NLP), has emerged as a key tool. However, prior state-of-the-art methods had significant limitations: autoregressive models (e.g., ProtGPT2) generate sequences left-to-right, which is ill-suited for proteins where functionally critical residues are dispersed throughout the sequence. Discrete diffusion models (e.g., EvoDiff) struggled to excel at both high-quality *de novo* (unconditional) generation and flexible conditional generation (e.g., inpainting) within a single framework. This work aims to solve the problem of creating a unified model capable of both tasks effectively.

3.  **Research Methodology:**
    The authors employed **Bayesian Flow Networks (BFNs)**, a generative framework that models the continuous parameters of a distribution over discrete data (amino acid sequences). The technical roadmap involved:
    - **Training:** Framed as a communication protocol where a "sender" (training data) sends progressively less noisy observations of a true sequence to a "receiver" (the neural network). The network (a 650M parameter transformer) learns to predict the parameters of the data distribution from these observations.
    - **Sampling:** A continuous-time denoising process starts from a random prior and iteratively refines the distribution parameters to generate a novel sequence.
    - **Data:** A curated dataset (UniProtCC) from UniProtKB, filtered for high-confidence protein existence.
    - **Model Variants:** A general protein model (**ProtBFN**) was trained on UniProtCC. It was then fine-tuned on antibody heavy-chain sequences from the Observed Antibody Space (OAS) database to create **AbBFN**.
    - **Conditional Generation:** For tasks like inpainting (e.g., predicting a missing CDR region), they used a Sequential Monte Carlo (SMC) sampling method on top of the unconditionally trained model, enabling zero-shot conditional generation.

4.  **Innovations:**
    - **Unified Framework:** Demonstrates that BFNs can effectively handle both *unconditional* generation of novel protein sequences and *arbitrary conditional* generation (e.g., inpainting) within a single model, a key advantage over previous paradigms.
    - **Novel Sampling:** Introduced a "Reconstructed ODE" (R-ODE) sampling method and "entropy encoding" (instead of time encoding) to stabilize generation and improve sample quality.
    - **Structural Coherence:** Generated sequences show high predicted structural confidence (pLDDT) and diversity, covering a broader swath of known protein space (proteome coverage) than baselines while maintaining novelty (low sequence identity to training data).
    - **Zero-shot Conditional Capability:** Shows that a model trained only for unconditional generation (AbBFN) can perform competitively on conditional tasks like antibody region inpainting without any task-specific training, challenging the need for specialized BERT-style models.

5.  **Applications:**
    - **De Novo Protein Design:** Generating entirely novel, functional protein sequences for therapeutic or enzymatic applications (e.g., novel enzymes, biosensors).
    - **Antibody Engineering:** Specifically, the design of antibody variable regions. AbBFN can be used to:
        - Generate novel, stable antibody heavy-chain candidates from scratch.
        - "Fix" or redesign specific regions (e.g., CDRs for affinity maturation, frameworks for stability) of existing antibody sequences through inpainting.
    - **Exploring Protein Space:** Systematically generating and studying proteins in uncharted regions of sequence space to uncover new folds and functions.

6.  **Limitations & Future Work:**
    - **Limitations:** The authors note that BFNs are an emerging technology with fundamental questions remaining. AbBFN underperforms specialized models on the highly diverse CDR-H3 region prediction, likely due to the unique biological processes (V(D)J recombination) generating its diversity, which a general protein model may not capture perfectly. The sampling process sometimes generates low-quality sequences, necessitating a filtering step.
    - **Future Work:** Suggested directions include extending BFNs to other biological sequences (RNA/DNA), developing *multimodal* BFNs that integrate sequence and structural data, and incorporating advanced sampling techniques from the diffusion literature to further improve performance.

7.  **Jargon Breakdown:**
    - **Bayesian Flow Networks (BFNs):** A class of generative models that learn a distribution over data by modeling continuous parameters of that distribution, rather than the data directly. This makes them naturally suited for discrete data like sequences. *Importance:* Provides a unified framework for generating and conditioning on discrete data. *Example:* ProtBFN uses BFNs to model the distribution of amino acids at each position in a protein.
    - **Inpainting:** A conditional generation task where a model is given a partial sequence and must generate the missing parts. *Importance:* Critical for protein design, e.g., redesigning a specific loop while keeping the rest of the protein fixed. *Example:* Using AbBFN to generate a missing CDR-H3 sequence given the rest of an antibody chain.
    - **pLDDT (predicted Local Distance Difference Test):** A per-residue confidence score (0-100) output by structure prediction models like AlphaFold or ESMFold. Higher scores indicate higher confidence in the predicted local structure. *Importance:* Used as a proxy metric for the structural plausibility and "foldability" of a generated protein sequence. *Example:* ProtBFN generates sequences with high mean pLDDT, suggesting they will fold into coherent structures.
    - **Unconditional vs. Conditional Generation:** *Unconditional generation* means creating samples from the entire learned distribution (e.g., generating a random protein). *Conditional generation* means creating samples that satisfy given constraints (e.g., generating a protein that contains a specific motif). *Importance:* A model that excels at both is more flexible and powerful for design tasks.
    - **Sequential Monte Carlo (SMC):** A sampling technique used for conditional generation with BFNs in this work. It uses multiple "particles" (guesses) that are resampled based on their agreement with the conditioning information. *Importance:* Enables the unconditionally trained BFN to perform complex conditional tasks like inpainting accurately.

8.  **Connections:**
    This paper represents a **foundational advancement** in the AIDD field, enabling a new capability rather than being an incremental improvement. It introduces a new model paradigm (BFNs) for biological sequences that challenges the dominance of autoregressive and diffusion models. Its key contribution is demonstrating that a single model architecture can achieve state-of-the-art performance in *both* unconditional generation *and* flexible conditional generation without task-specific training. This unification simplifies the model design process for protein engineers and opens new avenues for exploring sequence-structure-function relationships. It directly connects to and advances core AIDD themes like *de novo* protein design and antibody engineering.

The method presented in this paper has been open-sourced. Code is available at: https://github.com/instadeepai/protein-sequence-bfn
