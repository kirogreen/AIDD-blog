+++
date = '2025-10-06T22:22:42+08:00'
draft = false
title = "What Protein Language Models Learn During RL Fine-Tuning"
authors = "[Michael Yu]"
categories = ["literature review"]
tags = ['reinforcement learning', 'protein language models', 'antibody engineering', 'de novo design', 'direct preference optimization']
description = "daily summary of latest AIDD literature"
+++


## 1. Summary

**Original Title:** From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?
**Journal:** arXiv preprint
**Publication Date:** October 2, 2025
**DOI:** arXiv:2510.01571v1
**Primary Research Group/Institution:** Multi-institutional collaboration led by The Chinese University of Hong Kong, Peking University, and Stanford University.

This study systematically investigates whether reinforcement learning (RL) can help protein language models (PLMs) discover novel sequence-function relationships beyond their pre-training data. Through experiments across four protein design domains (antimicrobial peptide design, kinase optimization, antibody engineering, and inverse folding), the authors demonstrate that RL improves sampling efficiency but its effectiveness depends on a three-factor interaction: task difficulty, reward model accuracy, and policy capacity.

## 2. Background

The core scientific problem addresses the fundamental limitations of supervised learning approaches in protein design. Traditional methods struggle with: 1) optimizing complex non-differentiable biological objectives, 2) exploring beyond existing sequence-function mappings, and 3) integrating multi-objective criteria or experimental feedback. While protein language models have shown remarkable success through pre-training, they remain constrained by their training distributions. Reinforcement learning has demonstrated transformative potential in NLP by enabling models to discover emergent capabilities, but its capacity to unlock latent functional patterns in protein space remained underexplored.

## 3. Research Methodology

The authors employed a comprehensive evaluation framework across four biological systems:

1. **Protein Inverse Folding**: Used InstructPLM-7B as policy model with TM-Score as reward, implementing DPO with regularization
2. **Antimicrobial Peptide Design**: Employed Amphion-SFT with ApexMIC reward predictor, testing DPO, PPO, and GRPO algorithms
3. **Kinase Mutation**: Utilized ESM-2 architecture for multi-step mutations with experimental fitness as reward
4. **Antibody Optimization**: Developed improved ProtAttBA model for binding affinity prediction, using PPO and GRPO

The technical approach involved:
- Systematic comparison of RL algorithms (DPO, PPO, GRPO)
- Introduction of Expansion-Shrinkage Ratio (ESR) metric to quantify knowledge gain/loss
- Comprehensive evaluation using Pass@k metrics and biological plausibility measures
- Latent space analysis through UMAP visualizations

## 4. Innovations

**Key innovations compared to previous state-of-the-art:**

1. **Three-Factor Framework**: First principled framework identifying that RL effectiveness depends on task difficulty, reward accuracy, and policy capacity interaction
2. **Expansion-Shrinkage Ratio (ESR)**: Novel metric to quantify the trade-off between discovering new solutions and forgetting previous knowledge during RL fine-tuning
3. **Systematic Cross-Domain Evaluation**: First comprehensive study comparing RL effects across multiple protein design tasks using consistent evaluation metrics
4. **Improved Reward Modeling**: Enhanced antibody binding affinity prediction with combined regression and MLM objectives
5. **Task-Specific Algorithm Guidance**: Evidence-based recommendations for which RL algorithms work best for different protein design challenges

## 5. Applications

**Real-world applications in drug discovery and biological research:**

1. **Accelerated Antibody Optimization**: More efficient generation of high-affinity antibody variants for therapeutic development, particularly for cancer and infectious diseases
2. **Novel Antimicrobial Peptide Discovery**: Design of more potent AMPs to address antibiotic resistance crisis
3. **Enzyme Engineering**: Optimization of kinase and other enzyme properties for industrial biocatalysis and therapeutic applications
4. **De Novo Protein Design**: Improved inverse folding for creating proteins with novel functions not found in nature
5. **Experimental-Guided Design**: Framework for incorporating experimental feedback into computational design pipelines

## 6. Limitations & Future Work

**Acknowledged limitations:**
- RL often reduces diversity and novelty while improving specificity
- Performance heavily dependent on reward model accuracy (e.g., Spearman=0.47 for antibody task)
- ESR < 1.0 in many cases indicates net knowledge loss during fine-tuning
- Computationally intensive requiring significant resources

**Suggested future directions:**
- Extend to Diffusion/Flow Matching architectures beyond PLMs
- Explore protein structure and sequence-structure co-design
- Incorporate additional RL algorithms (e.g., MCTS)
- Improve reward model accuracy and robustness
- Develop better regularization techniques to maintain diversity

## 7. Jargon Breakdown

**Protein Language Models (PLMs):** AI models trained on protein sequences to understand and generate biologically plausible proteins. Example: ESM-2 or ProtGPT2 that can predict protein function or generate novel sequences.

**Reinforcement Learning (RL):** Machine learning approach where an agent learns to make decisions by receiving rewards or penalties. Example: Training a model to generate better antibodies by rewarding improved binding affinity.

**Direct Preference Optimization (DPO):** RL technique that learns from preference pairs without explicit reward modeling. Example: Showing the model pairs of proteins where one has better activity and learning to prefer it.

**Pass@k:** Evaluation metric measuring the probability of generating at least one successful sequence in k attempts. Example: Pass@100 = 0.8 means 80% chance of getting a good protein in 100 tries.

**TM-Score:** Metric for assessing structural similarity between proteins (1.0 = perfect match). Example: Used to evaluate how well a generated sequence folds into the target structure.

## 8. Connections

This paper provides a significant **incremental improvement** to the AIDD field by offering a principled framework for applying RL to protein design. It doesn't enable fundamentally new capabilities but provides crucial insights into how to effectively use existing RL techniques with PLMs. The research connects to broader AIDD by:

- Establishing guidelines for when and how to apply RL in protein design pipelines
- Providing diagnostic tools (like ESR) to evaluate RL training effectiveness
- Demonstrating the importance of reward model quality in biological applications
- Offering practical guidance for resource allocation in RL-based protein engineering

## 9. Note

The method presented in this paper has been open-sourced. Implementation is available at github (specific repository URL not provided in the document).
